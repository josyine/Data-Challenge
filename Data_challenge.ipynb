{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0229f703",
   "metadata": {},
   "source": [
    "étape_0_bis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bd719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du fichier d'entrée et de sortie\n",
    "input_file = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/instructions_list.txt\"\n",
    "output_file = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/instructions_list_cleaned.txt\"\n",
    "\n",
    "# Lire le fichier et filtrer les lignes\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Filtrer les lignes ne commençant pas par un chiffre et ne finissant pas par \"]\"\n",
    "cleaned_lines = [line for line in lines if not line.strip().startswith(tuple(\"0123456789\")) and not line.strip().endswith(\"]\")]\n",
    "\n",
    "# Enregistrer le nouveau fichier\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(cleaned_lines)\n",
    "\n",
    "# Afficher le nombre de lignes restantes\n",
    "print(f\"Nombre de lignes dans le nouveau fichier : {len(cleaned_lines)}\")\n",
    "print(f\"Fichier nettoyé enregistré dans : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa9e78",
   "metadata": {},
   "source": [
    "étape_0_instructions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def extract_all_instructions(folder_path, output_file):\n",
    "    instructions_set = set()\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):  # Vérifie si c'est un fichier JSON\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    lines = f.readlines()  # Lire le fichier ligne par ligne\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        # Extraire le mot après le 2ème \":\"\n",
    "                        parts = line.split(\":\")\n",
    "                        if len(parts) > 2:\n",
    "                            word_after_second_colon = parts[2].strip().split()[0]  # Prendre le premier mot après le deuxième \":\"\n",
    "                            instructions_set.add(word_after_second_colon)  # Utiliser .add() pour un seul mot\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur de lecture du fichier {filename} : {e}\")\n",
    "\n",
    "    # Trier la liste et l'enregistrer dans un fichier\n",
    "    sorted_instructions = sorted(instructions_set)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for instr in sorted_instructions:\n",
    "            f_out.write(instr + \"\\n\")\n",
    "    \n",
    "    print(f\"Nombre total d'instructions uniques : {len(sorted_instructions)}\")\n",
    "    print(f\"Liste des instructions enregistrée dans : {output_file}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "folder_path = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/folder_training_set\"\n",
    "output_file = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/instructions_list.txt\"\n",
    "\n",
    "extract_all_instructions(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657d957",
   "metadata": {},
   "source": [
    "étape_1_création_features_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def create_features_dataframe(folder_path, instructions_list, output_csv):\n",
    "    # Charger la liste des instructions depuis le fichier\n",
    "    with open(instructions_list, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_instructions = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # Initialiser une liste pour stocker les features\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):  # Vérifie si c'est un fichier JSON\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_id = filename.replace(\".json\", \"\")  # Supprimer .json\n",
    "            \n",
    "            features = {instr: 0 for instr in all_instructions}  # Init à 0 pour toutes les instructions\n",
    "            features[\"filename\"] = file_id  # Ajouter le nom du fichier SANS .json\n",
    "            \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    lines = f.readlines()  # Lire le fichier ligne par ligne\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        # Extraire le mot après le 2ème \":\"\n",
    "                        parts = line.split(\":\")\n",
    "                        if len(parts) > 2:\n",
    "                            word_after_second_colon = parts[2].strip().split()[0]  # Prendre le premier mot après le deuxième \":\"\n",
    "                            \n",
    "                            if word_after_second_colon in features:  # Vérifier si l'instruction est dans la liste\n",
    "                                features[word_after_second_colon] = 1  # Marquer sa présence\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur de lecture du fichier {filename} : {e}\")\n",
    "\n",
    "            data.append(features)\n",
    "\n",
    "    # Convertir en DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Sauvegarder en CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"DataFrame enregistré : {output_csv}\")\n",
    "    print(df.head())  # Afficher un aperçu\n",
    "\n",
    "# Exemple d'utilisation\n",
    "folder_path = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/folder_training_set\"\n",
    "instructions_list = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/instructions_list_cleaned.txt\"\n",
    "output_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/features_train.csv\"\n",
    "\n",
    "create_features_dataframe(folder_path, instructions_list, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628db287",
   "metadata": {},
   "source": [
    "étape_2_bis_fusion_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_with_labels(features_csv, labels_csv, output_csv):\n",
    "    # Charger les fichiers CSV\n",
    "    df_features = pd.read_csv(features_csv)\n",
    "    df_labels = pd.read_csv(labels_csv)  # Vérifier le bon séparateur\n",
    "    \n",
    "    # Renommer la colonne `filename` du fichier features pour qu'elle corresponde à `name`\n",
    "    df_features.rename(columns={\"filename\": \"name\"}, inplace=True)\n",
    "\n",
    "    # Fusionner les deux DataFrames sur `filename`\n",
    "    df_final = df_labels.merge(df_features, on=\"name\", how=\"inner\")\n",
    "\n",
    "    # Sauvegarder le DataFrame fusionné\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"DataFrame fusionné enregistré : {output_csv}\")\n",
    "    print(df_final.head())  # Afficher un aperçu\n",
    "\n",
    "features_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/features_train_inst.csv\"\n",
    "labels_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/train_data.csv\"\n",
    "output_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/new_train_data.csv\"\n",
    "\n",
    "merge_features_with_labels(features_csv, labels_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8c943",
   "metadata": {},
   "source": [
    "étape_2_fusion_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_with_labels(features_csv, labels_csv, output_csv):\n",
    "    # Charger les fichiers CSV\n",
    "    df_features = pd.read_csv(features_csv)\n",
    "    df_labels = pd.read_csv(labels_csv, sep=\";\")  # Vérifier le bon séparateur\n",
    "    \n",
    "    # Renommer la colonne `filename` du fichier features pour qu'elle corresponde à `name`\n",
    "    df_features.rename(columns={\"filename\": \"name\"}, inplace=True)\n",
    "\n",
    "    # Fusionner les deux DataFrames sur `filename`\n",
    "    df_final = df_labels.merge(df_features, on=\"name\", how=\"inner\")\n",
    "\n",
    "    # Sauvegarder le DataFrame fusionné\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"DataFrame fusionné enregistré : {output_csv}\")\n",
    "    print(df_final.head())  # Afficher un aperçu\n",
    "\n",
    "features_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/features_train.csv\"\n",
    "labels_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/training_set_metadata.csv\"\n",
    "output_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/train_data.csv\"\n",
    "\n",
    "merge_features_with_labels(features_csv, labels_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449dd5b",
   "metadata": {},
   "source": [
    "étape_3_création_features_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d00cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def create_features_dataframe(folder_path, instructions_list, output_csv):\n",
    "    # Charger la liste des instructions depuis le fichier\n",
    "    with open(instructions_list, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_instructions = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # Initialiser une liste pour stocker les features\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):  # Vérifie si c'est un fichier JSON\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_id = filename.replace(\".json\", \"\")  # Supprimer .json\n",
    "            \n",
    "            features = {instr: 0 for instr in all_instructions}  # Init à 0 pour toutes les instructions\n",
    "            features[\"filename\"] = file_id  # Ajouter le nom du fichier SANS .json\n",
    "            \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    lines = f.readlines()  # Lire le fichier ligne par ligne\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        # Extraire le mot après le 2ème \":\"\n",
    "                        parts = line.split(\":\")\n",
    "                        if len(parts) > 2:\n",
    "                            word_after_second_colon = parts[2].strip().split()[0]  # Prendre le premier mot après le deuxième \":\"\n",
    "                            \n",
    "                            if word_after_second_colon in features:  # Vérifier si l'instruction est dans la liste\n",
    "                                features[word_after_second_colon] = 1  # Marquer sa présence\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur de lecture du fichier {filename} : {e}\")\n",
    "\n",
    "            data.append(features)\n",
    "\n",
    "    # Convertir en DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Sauvegarder en CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"DataFrame enregistré : {output_csv}\")\n",
    "    print(df.head())  # Afficher un aperçu\n",
    "\n",
    "# Exemple d'utilisation\n",
    "folder_path = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/folder_test_set\"\n",
    "instructions_list = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/instructions_list_cleaned.txt\"\n",
    "output_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/features_test.csv\"\n",
    "\n",
    "create_features_dataframe(folder_path, instructions_list, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e992fc",
   "metadata": {},
   "source": [
    "étape_4_bis_fusion_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_with_labels(features_csv, labels_csv, output_csv):\n",
    "    # Charger les fichiers CSV\n",
    "    df_features = pd.read_csv(features_csv)\n",
    "    df_labels = pd.read_csv(labels_csv)  # Vérifier le bon séparateur\n",
    "    \n",
    "    # Renommer la colonne `filename` du fichier features pour qu'elle corresponde à `name`\n",
    "    df_features.rename(columns={\"filename\": \"name\"}, inplace=True)\n",
    "\n",
    "    # Fusionner les deux DataFrames sur `filename`\n",
    "    df_final = df_labels.merge(df_features, on=\"name\", how=\"inner\")\n",
    "\n",
    "    # Sauvegarder le DataFrame fusionné\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"DataFrame fusionné enregistré : {output_csv}\")\n",
    "    print(df_final.head())  # Afficher un aperçu\n",
    "\n",
    "features_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/features_test_inst.csv\"\n",
    "labels_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/test_data.csv\"\n",
    "output_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/new_test_data.csv\"\n",
    "\n",
    "merge_features_with_labels(features_csv, labels_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f20ad",
   "metadata": {},
   "source": [
    "étape_4_fusion_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1182df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_with_labels(features_csv, labels_xlsx, output_csv):\n",
    "    # Charger les fichiers CSV et Excel\n",
    "    df_features = pd.read_csv(features_csv)\n",
    "    df_labels = pd.read_excel(labels_xlsx)  # Lire le fichier Excel\n",
    "    \n",
    "    # Vérifier si la colonne `fikename` existe dans df_features\n",
    "    if \"filename\" in df_features.columns:\n",
    "        df_features.rename(columns={\"filename\": \"name\"}, inplace=True)\n",
    "    else:\n",
    "        print(\"Erreur : La colonne 'filename' est introuvable dans le fichier features.\")\n",
    "        return\n",
    "\n",
    "    # Vérifier si la colonne `name` existe dans df_labels\n",
    "    if \"name\" not in df_labels.columns:\n",
    "        print(\"Erreur : La colonne 'name' est introuvable dans le fichier labels.\")\n",
    "        return\n",
    "\n",
    "    # Fusionner les deux DataFrames sur `name`\n",
    "    df_final = df_labels.merge(df_features, on=\"name\", how=\"inner\")\n",
    "\n",
    "    # Sauvegarder le DataFrame fusionné\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"DataFrame fusionné enregistré : {output_csv}\")\n",
    "    print(df_final.head())  # Afficher un aperçu\n",
    "\n",
    "features_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/features_test.csv\"\n",
    "labels_xlsx = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/test_set_metadata_to_predict.xlsx\"\n",
    "output_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/test_data.csv\"\n",
    "\n",
    "merge_features_with_labels(features_csv, labels_xlsx, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f71d5b",
   "metadata": {},
   "source": [
    "étape_5_prédiction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Fonction pour entraîner XGBoost\n",
    "def train_xgboost(train_csv):\n",
    "    # Charger le dataset\n",
    "    df = pd.read_csv(train_csv)\n",
    "\n",
    "    # Définir les features (X) et les labels (Y)\n",
    "    label_columns = df.columns[4:454]  # Colonnes 5 à 454 = Labels\n",
    "    feature_columns = df.columns[454:]  # Colonnes 455+ = Features extraites des instructions\n",
    "\n",
    "    X = df[feature_columns].astype(np.float32)  # Convertir en float32 pour XGBoost\n",
    "    Y = df[label_columns]  # Labels à prédire\n",
    "\n",
    "    models = {}  # Stocker les modèles entraînés\n",
    "    f1_scores = {}  # Stocker les scores\n",
    "\n",
    "    # Séparer les données en train/test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    for label in tqdm(Y.columns, desc=\"Training XGBoost Models\"):\n",
    "        print(f\"Entraînement du modèle pour {label}...\")\n",
    "\n",
    "        # Création du modèle XGBoost\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\", eval_metric=\"logloss\", \n",
    "            base_score=0.5  # Spécifier la valeur de base_score dans l'intervalle [0, 1]\n",
    "        )\n",
    "\n",
    "        # Entraînement\n",
    "        model.fit(X_train, Y_train[label])\n",
    "\n",
    "        # Prédiction et évaluation\n",
    "        Y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(Y_test[label], Y_pred, average=\"macro\")\n",
    "\n",
    "        print(f\"Macro F1 Score pour {label} : {f1:.4f}\")\n",
    "        models[label] = model\n",
    "        f1_scores[label] = f1\n",
    "\n",
    "    print(\"\\nEntraînement terminé !\")\n",
    "    return models\n",
    "\n",
    "# Fonction pour entraîner RandomForest\n",
    "def train_random_forest(train_csv):\n",
    "    # Charger le dataset\n",
    "    df = pd.read_csv(train_csv)\n",
    "\n",
    "    # Définir les features (X) et les labels (Y)\n",
    "    label_columns = df.columns[4:454]  # Colonnes 5 à 454 = Labels\n",
    "    feature_columns = df.columns[454:]  # Colonnes 455+ = Features extraites des instructions\n",
    "\n",
    "    X = df[feature_columns].astype(np.float32)  # Convertir en float32 pour RandomForest\n",
    "    Y = df[label_columns]  # Labels à prédire\n",
    "\n",
    "    models = {}  # Stocker les modèles entraînés\n",
    "    f1_scores = {}  # Stocker les scores\n",
    "\n",
    "    # Séparer les données en train/test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    for label in tqdm(Y.columns, desc=\"Training Random Forest Models\"):\n",
    "        print(f\"Entraînement du modèle pour {label}...\")\n",
    "\n",
    "        # Création du modèle Random Forest\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "        # Entraînement\n",
    "        model.fit(X_train, Y_train[label])\n",
    "\n",
    "        # Prédiction et évaluation\n",
    "        Y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(Y_test[label], Y_pred, average=\"macro\")\n",
    "\n",
    "        print(f\"Macro F1 Score pour {label} : {f1:.4f}\")\n",
    "        models[label] = model\n",
    "        f1_scores[label] = f1\n",
    "\n",
    "    print(\"\\nEntraînement terminé !\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_test_data(models, test_csv, model_name):\n",
    "    # Charger les données de test\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "\n",
    "    # Définir les features utilisées (colonnes 455+)\n",
    "    feature_columns = df_test.columns[454:]\n",
    "    X_test = df_test[feature_columns].astype(np.float32)  # Convertir en float32\n",
    "\n",
    "    # Prédire uniquement pour les labels ayant un modèle\n",
    "    for label, model in tqdm(models.items(), desc=f\"Predicting on {model_name} Test Data\"):\n",
    "        print(f\"Prédiction pour {label}...\")\n",
    "        df_test[label] = model.predict(X_test).astype(int)  # Prédiction des valeurs\n",
    "\n",
    "    # Sauvegarder les prédictions dans le même fichier\n",
    "    df_test.to_csv(f\"test_predictions_{model_name}.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nPrédictions mises à jour dans : test_predictions_{model_name}.csv\")\n",
    "\n",
    "\n",
    "# Fichiers d'entrée\n",
    "train_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/new_train_data.csv\"\n",
    "test_csv = \"C:/Users/smohun/OneDrive - American International Group, Inc/Desktop/Data Challenge/new_test_data.csv\"\n",
    "\n",
    "# Entraînement et prédiction avec XGBoost\n",
    "# xgboost_models = train_xgboost(train_csv)\n",
    "# predict_test_data(xgboost_models, test_csv, \"xgboost\")\n",
    "\n",
    "# Entraînement et prédiction avec RandomForest\n",
    "rf_models = train_random_forest(train_csv)\n",
    "predict_test_data(rf_models, test_csv, \"rf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
